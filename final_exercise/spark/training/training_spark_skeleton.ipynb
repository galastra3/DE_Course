{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e30b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/02 21:23:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b9afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb = spark.read.text(\"./data/sentiment/imdb_labelled.txt\")\n",
    "df_amazon = spark.read.text(\"./data/sentiment/amazon_cells_labelled.txt\")\n",
    "df_yelp = spark.read.text(\"./data/sentiment/yelp_labelled.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279bdf38",
   "metadata": {},
   "source": [
    "- In the `readme.txt` file (the same directory where the data are), read how to parse the text.\n",
    "- - Hint: You might want to change the block above a bit.\n",
    "- Union all the three dataframes to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union all the dataframes and parse them to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you split to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a39c5",
   "metadata": {},
   "source": [
    "After we split to train and test, we will build a pipeline to extract the features from the text.\n",
    "\n",
    "If you remember from the lectures, you can't do analysis on the words themselves, since they have no information.\n",
    "\n",
    "So, we need to have an informative feature for the words.\n",
    "\n",
    "We will choose counting as a basic feature that works.\n",
    "\n",
    "_(Meaning that the value of the word will be the count of how many times it occurs in the dataset)_\n",
    "\n",
    "So, we will use [`pyspark.ml.feature.CountVectorizer`](https://spark.apache.org/docs/latest/ml-features#countvectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9ed42",
   "metadata": {},
   "source": [
    "But first, we need to split the words.\n",
    "And we want to do it smart, not just by `.split()`.\n",
    "\n",
    "So, we will use [`pyspark.ml.feature.Tokenizer`](https://spark.apache.org/docs/latest/ml-features#tokenizer).\n",
    "\n",
    "Also, *Note*: you need `StringIndexer` to parse the label to be an index and not a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5ba960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "counter = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol=\"count_words\")\n",
    "label_string2ix = StringIndexer(inputCol=\"target\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7cb692",
   "metadata": {},
   "source": [
    "Next, we will use [`pyspark.ml.classification.LogisticRegression`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html) for our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcddcc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=400)\n",
    "# continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2b0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `pyspark.ml.pipeline` of the feature extractors and the model.\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f379aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the train\n",
    "# (like we did with the ML exercises earlier in the course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988e190",
   "metadata": {},
   "source": [
    "We need some benchmarks to understand how well is our model.\n",
    "\n",
    "We'll use [`pyspark.ml.evaluation.BinaryClassificationEvaluator`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html) to evaluate the model on train and on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4691ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# continue..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
